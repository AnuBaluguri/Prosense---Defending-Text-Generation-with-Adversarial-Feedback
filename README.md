# Prosense: Defending Text Generation with Adversarial Feedback

Prosense is a multi-stage training framework designed to improve the robustness of large language models (LLMs) against adversarial inputs. It fine-tunes quantized LLMs using clean and adversarially augmented datasets, evaluates model performance via reasoning-based judgments, and refines the model through structured adversarial feedback using **Graph-of-Thought (GOT)** parsing.

> ðŸš€ **Model Base**: `unsloth/mistral-7b-bnb-4bit`  
> ðŸ§  **Goal**: Increase factuality and robustness through adversarial feedback  
> ðŸ§ª **Datasets**: Open-Instruct, TruthfulQA, GOT-parsed failures

---

## ðŸ“ Project Structure

```
Prosense-Adversarial-Robustness/
â”‚
â”œâ”€â”€ Phase1_Clean_FineTuning/
â”‚   â””â”€â”€ phase1.ipynb
â”‚
â”œâ”€â”€ Phase2_Adversarial_Hybrid/
â”‚   â”œâ”€â”€ HybridDataCreation.ipynb
â”‚   â”œâ”€â”€ MergingWithHybridDataset.ipynb
â”‚   â””â”€â”€ Phase2_Final.ipynb
â”‚
â”œâ”€â”€ Phase3_Evaluation_Parsing/
â”‚   â”œâ”€â”€ Phase3_1_Collect_TruthfulQA_Responses.ipynb
â”‚   â”œâ”€â”€ Phase3_2_Judge_and_Filter_Failures.ipynb
â”‚   â”œâ”€â”€ Phase3_3_Parse_GOT_Graph_By_LLaMA.ipynb
â”‚   â””â”€â”€ Phase3_Final.ipynb
â”‚
â””â”€â”€ Phase3_Level2_Refinement/
    â”œâ”€â”€ Phase4_1_Collect_Level1_Responses.ipynb
    â”œâ”€â”€ Phase4_2_Judge_Parse_Level1_By_LLaMA.ipynb
    â”œâ”€â”€ Phase4_3_Level2_Finetune.ipynb
    â”œâ”€â”€ Phase4_4_Collect_Level2_Responses.ipynb
    â”œâ”€â”€ Phase4_5_Judge_Level2_Responses.ipynb
    â””â”€â”€ Phase4_6_Reasoning_Graph_Visualization.ipynb
```

---

## ðŸ“Œ Methodology Overview

### ðŸ”¹ Phase 1 â€“ Baseline Fine-Tuning
Fine-tunes Mistral-7B (4-bit quantized) on the Open-Instruct dataset.

> âœ… Output: `mistral-prosense-clean` â€“ Clean model baseline

---

### ðŸ”¹ Phase 2 â€“ Adversarial Hybrid Fine-Tuning
- Generates adversarial examples from clean data
- Merges clean + adversarial sets
- Re-trains model to enhance robustness

> âœ… Output: `mistral-prosense-hybrid` â€“ Robustness-enhanced model

---

### ðŸ”¹ Phase 3 â€“ TruthfulQA Evaluation + GOT Parsing
- Evaluates the hybrid model on TruthfulQA
- Judges failed samples with another LLM (Gemma or LLaMA)
- Parses failures into **Graph-of-Thought (GOT)** reasoning format
- Fine-tunes again using these structured reasoning examples

> âœ… Output: `mistral-prosense-parsed` â€“ GOT-aware model

---

### ðŸ”¹ Phase 3 Again â€“ Level 2 Feedback Cycle
- Re-evaluates the parsed model (Level 1)
- Parses second round of failed cases
- Final fine-tuning with deeper feedback
- Visualizes reasoning improvement via GOT graphs

> âœ… Output: `mistral-finetuning-again` â€“ Final robust model

---

## âš™ï¸ Dependencies

Install required packages:

```bash
pip install transformers accelerate datasets bitsandbytes unsloth textattack torch
```

> Python 3.10+ and a 24GB+ VRAM GPU are recommended for fine-tuning.

---

## ðŸš€ How to Run

Run notebooks in order:

### Phase 1: Clean Fine-Tuning
1. `phase1.ipynb`

### Phase 2: Adversarial Training
2. `HybridDataCreation.ipynb`
3. `MergingWithHybridDataset.ipynb`
4. `Phase2_Final.ipynb`

### Phase 3: Evaluation + GOT Parsing
5. `Phase3_1_Collect_TruthfulQA_Responses.ipynb`
6. `Phase3_2_Judge_and_Filter_Failures.ipynb`
7. `Phase3_3_Parse_GOT_Graph_By_LLaMA.ipynb`
8. `Phase3_Final.ipynb`

### Phase 4: Level 2 Feedback Cycle
9. `Phase4_1_Collect_Level1_Responses.ipynb`
10. `Phase4_2_Judge_Parse_Level1_By_LLaMA.ipynb`
11. `Phase4_3_Level2_Finetune.ipynb`
12. `Phase4_4_Collect_Level2_Responses.ipynb`
13. `Phase4_5_Judge_Level2_Responses.ipynb`
14. `Phase4_6_Reasoning_Graph_Visualization.ipynb`

---

## ðŸ“Š Evaluation Results 
You can add metrics here if you tracked model performance across stages, such as:

- TruthfulQA Pass Rate: 35% â†’ 55% (Post Phase 3)
- Adversarial Robustness Improvement: +30%

---

## ðŸ“„ License

This project is licensed under the [MIT License](LICENSE).  
Feel free to use, adapt, or distribute with proper attribution.

---

